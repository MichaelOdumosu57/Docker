learning docker
Docker is a software which provides centralized platform to execute your application. It wraps software components into a complete standardized unit which contains everything require to run.

references
1.https://docs.docker.com/install/linux/docker-ce/ubuntu/#os-requirements
2. https://docs.docker.com/get-started/
3 https://docs.docker.com/compose/install/
4.https://docs.docker.com/docker-cloud/infrastructure/byoh/


Docker provides the facility to run an application in a isolated environment which is called container.

--- Docker container is not any specific platform. It can run on any computer, on any infrastructure and in any cloud.

Docker Engine
    
    client-server app that has

    daemon- a long running server process
    REST API - talks to daemon and tells it what to do
    command line client interface

    it manages
    networks, containers, images and data volumes

Docker Features

    configures easily and fast becuase it does not depend on infrastructure
    reduce resources needed to execute
     Each container is independent to another and allows us to execute any kind of application.
    swarm - It is a clustering and scheduling tool for Docker containers.
    It routes the incoming requests for published ports on available nodes to an active container. This feature enables the connection even if there is no task is running on the node.
    Services is a list of tasks that lets us specify the state of the container inside a cluster
    It allows us to save secrets into the swarm itself and then choose to give services access to certain secrets.

Docker Architecture

    Docker_Host: It contains Containers, Images, and Docker daemon. It provides complete environment to execute and run your application.
    Client: Docker provides Command Line Interface (CLI) tools to client to interact with Docker daemon. Client can build, run and stop application. Client can also interact to Docker_Host remotely.
     Registry: It is global repository of images. You can access and use these images to run your application in Docker environment.

    docker daemon
    It is a process which is used to listen for Docker API requests. It also manages Docker objects like: images, container, network etc. A daemon can also communicate with other daemons to manage Docker services.


    The Docker client
    The Docker client is the primary way that many Docker users interact with Docker. When we use commands such as docker run, the client sends these commands to docker d, which carries them out. The docker command uses the Docker API.

Docker Installation

    for ubuntu 16.04 xenial LTS
        only 64 bit
        requires linux kernel > 3.10

        to check kernel type in uname -r

    to install
        sudo apt-get update
        sudo apt-get install apt-transport-https ca-certificates

tried to copy and paste sudo apt-key adv \
--keyserver hkp://ha.pool.sks-keyservers.net:80 \
--recv-keys 58118E89F3A912897C070ADBF76221572C52609D

got this

Executing: /tmp/tmp.DGR8vsVyRO/gpg.1.sh
gpg: can't open ` '

tried this
    sudo apt-key adv \ --keyserver hkp://ha.pool.sks-keyservers.net:80 \ --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
got
usage: gpg [options] [filename]
    
    tried this

    gpg is not used right , have to do some research
    
    DID THIS INSTEAD

1/.
        sudo apt-get remove docker docker-engine docker.io
        sudo apt-get update
        sudo apt-get install \
            apt-transport-https \
            ca-certificates \
            curl \
            software-properties-common
         curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

verrify you have key with fingerprint 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88
        
        sudo apt-key fingerprint 0EBFCD88
        sudo add-apt-repository \
            "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
            $(lsb_release -cs) \
            stable"
        sudo apt-get update
        sudo apt-get install docker-ce
        sudo docker run hello-world

SUCCESS

Docker Container and Image

    Docker container runs sepereate but can access things running in other places

    An image is a read-only template with instructions for creating a Docker container. A docker image is described in text file called a Dockerfile, which has a simple, well-defined syntax. An image does not have states and never changes.

    docker run hello-world
    The above command docker run hello-world has three parts.
    
    1) docker: It is docker engine and used to run docker program. It tells to the operating system that you are running docker program.
    
    2) run: This subcommand is used to create and run a docker container.
    
    3) hello-world: It is a name of an image. You need to specify the name of an image which is to load into the container.

Docker Dockerfile

    dockerfile - A Dockerfile is a text document that contains commands that are used to assemble an image
    
    You can use the -f flag with docker build to point to a Dockerfile anywhere in your file system.
    docker build -f /path/to/a/Dockerfile

    Instructions
    
    Docker runs instructions of Dockerfile in top to bottom order. The first instruction must be FROM in order to specify the Base Image.


    FROM - This instruction is used to set the Base Image for the subsequent instructions. A valid Dockerfile must have FROM as its first instruction.

        FROM ubuntu

    LABEL - We can add labels to an image to organize images of our project. We need to use LABEL instruction to set label for the image.

        LABEL vendorl = "JavaTpoint"

    RUN - This instruction is used to execute any command of the current image.

        RUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME'

    CMD - This is used to execute application by the image. We should use CMD always in the following form
    
        CMD ["executable", "param1", "param2"?]

    COPY - This instruction is used to copy new files or directories from source to the filesystem of the container at the destination.

        COPY abc/ /xyz


        --The source path must be inside the context of the build, docker does not know about external directories


    WORKDIR -The WORKDIR is used to set the working directory for any RUN, CMD and COPY instruction that follows it     in the Dockerfile. If work directory does not exist, it will be created by default.
        We can use WORKDIR multiple times in a Dockerfile.


        WORKDIR /var/www/html


Docker Java Application Example

     mkdir  java-docker-app
        
cant use https://www.javatpoint.com/docker-java-example, non compatible

2.
    

## List Docker CLI commands
docker
docker container --help

## Display Docker version and info
docker --version
docker version
docker info

## Excecute Docker image
docker run hello-world

## List Docker images
docker image ls

## List Docker containers (running, all, all in quiet mode)
docker container ls
docker container ls -all
docker container ls -a -q
        

With Docker, scaling your application is a matter of spinning up new executables, not running heavy VM hosts.

using container

    make a dir make a dockerfile inside it and add dockerfile commands

    proxys can block things use this code to enable view before the call to pip

    --------------------------------------------------------------
    # Set proxy server, replace host:port with values for your servers
    ENV http_proxy host:port
    ENV https_proxy host:port

    --------------------------------------------------------------


    in /home/autospark/My_Computer/Docker/docker-python-app
    in app.py docker is looking for redis but redis is not in the system
    but you have setup the app

    docker build -t friendlyhello .

-t has a friendly name

    docker images

where your images are

    docker run -p 4000:80 friendlyhello

Run the app, mapping your machine’s port 4000 to the container’s published port 80 using -p

Python is serving your app at http://0.0.0.0:80. But that message is coming from inside the container, which doesn’t know you mapped port 80 of that container to 4000, making the correct URL http://localhost:4000.

    curl http://localhost:4000

do this to see result in cmd


This port remapping of 4000:80 is to demonstrate the difference between what you EXPOSE within the Dockerfile, and what you publish using docker run -p.


    docker run -d -p 4000:80 friendlyhello
    docker container ls

run the app in the background, in detached mode:
You get the long container ID for your app and then are kicked back to your terminal. Your container is running in the background. You can also see the abbreviated container ID with docker container ls (and both work interchangeably when running commands):

    docker container stop [container_id]

Now use docker container stop to end the process, using the CONTAINER ID, like so:

folder  /home/autospark/My_Computer/Docker/docker-python-app


need to know how to push to registries when you want to deploy containers to production. upload our built image and run it somewhere else

A registry is a collection of repositories, and a repository is a collection of images—sort of like a GitHub repository, except the code is already built. An account on a registry can create many repositories. The docker CLI uses Docker’s public registry by default.


                                                            To set up registry

make an account
https://cloud.docker.com/

Log in to the Docker public registry on your local machine.
    docker login


The notation for associating a local image with a repository on a registry is username/repository:tag. The tag is optional, but recommended, since it is the mechanism that registries use to give Docker images a version.

Give the repository and tag meaningful names for the context, such as get-started:part2. This puts the image in the get-started repository and tag it as part2.

    docker tag image username/repository:tag



to see it

    docker image ls



to publish
    
    docker push username/repository:tag

From now on, you can use docker run and run your app on any machine with this command: If the image isn’t available locally on the machine, Docker pulls it from the repository.

    docker run -p 4000:80 username/repository:tag




                                            Get Started, Part 3: Services
    
    need docker compose
3
    Compose is a tool for defining and running multi-container Docker applications
    
    download the Docker Compose binary from the Compose repository release page on GitHub.

        sudo curl -L https://github.com/docker/compose/releases/download/1.19.0-rc3/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
    
    apply execution to binary
    
        sudo chmod +x /usr/local/bin/docker-compose
    
    installation test
    
        docker-compose --version

Services
In a distributed application, different pieces of the app are called “services.”

to manage services you just need a yml file

    make a docker-compose.yml whereever, replaced that image in your personal registry in the info in the example

    it tells docker to

        >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Pull the image we uploaded in step 2 from the registry.

Run 5 instances of that image as a service called web, limiting each one to use, at most, 10% of the CPU (across all cores), and 50MB of RAM.

Immediately restart containers if one fails.

Map port 80 on the host to web’s port 80.

Instruct web’s containers to share port 80 via a load-balanced network called webnet. (Internally, the containers themselves publish to web’s port 80 at an ephemeral port.)

Define the webnet network with the default settings (which is a load-balanced overlay network).

        >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

---------------------------run your new load-balanced app

    docker swarm init

give app a name getstartedlab

    sudo docker stack deploy -c docker-compose.yml getstartedlab

see the load balanced app as a service

    sudo docker service ls

A single container running in a service is called a task. Tasks are given unique IDs that numerically increment, up to the number of replicas you defined in docker-compose.yml. List the tasks for your service:

    sudo docker service ps getstartedlab_web
    OR
    sudo docker container ls -q

go to browser and refresh several times to see the result

container id changes matching id from docker container ls -q

--------------------------Scale the app

You can scale the app by changing the replicas value in docker-compose.yml,
docker changes in place without having to do anything too see result docker container ls -q

-----------------------Take down the app and the swarm

Take the app down with docker stack rm:

    docker stack rm getstartedlab

take down swarm
    
    docker swarm leave --force


    FOLDER: docker_services


                                    Get Started, Part 4: Swarms

Multi-container, multi-machine applications are made possible by joining multiple machines into a “Dockerized” cluster called a swarm.

A swarm is a group of machines that are running Docker and joined into a cluster. After that has happened, you continue to run the Docker commands you’re used to, but now they are executed on a cluster by a swarm manager.

After joining a swarm, they are  called  nodes.

swarm managers have different ways of running things

basically your computer can turn into a control for a whole set of computers


--------------------------Set up your swarm

to become a manager

    docker swarm init

to join a swarm

    docker swarm join

you can try this out by using virtual machines

    sudo docker-machine create --driver virtualbox myvm1
    sudo docker-machine create --driver virtualbox myvm2

----
however myvm1 shoes up as not running, obviously because the vt-x/ functionality was disabled in bios
so im using myvm3 as a replacement
---

command to list the machines and get their IP addresses

    sudo docker-machine ls

send command to VM

    docker-machine ssh [VM MACHINE NAME]
if trouble
    docker-machine --native-ssh ssh  [VM MACHINE NAME]
docker-machine --native-ssh ssh  myvm2 "docker swarm init --advertise-addr <myvm2 ip>"

got
sh: syntax error: unexpected end of file
exit status 2

    made my computer the swarm manager, and made the other vm join me

    sudo docker-machine --native-ssh ssh [VM MACHINE] "sudo docker swarm join --token SWMTKN-1-4n8dosttl2fljalp9nmvvz3wi95xbvt0ffn1socerbvxbbdkzf-57fov9ovaplmwanme4r7owilr 192.168.1.4:2377"


TIP: never used ports 2377,2376

if swarm was the manager you run commands through it using

    docker-machine --native-ssh  ssh [VM MACHINE NAME] "[command] "

to see the activity from the swarm manager run
    
    sudo docker node ls

if you want to start over

    docker swarm leave

do what you did in part 3 and deploy your swarm


------------------------Configure a docker-machine shell to the swarm manager

This method works better for the next step because it allows you to use your local docker-compose.yml file to deploy the app “remotely” without having to copy it anywhere.

    sudo docker-machine env [VM MACHINE]

    eval $(docker-machine env myvm2)

    but you must be root so start the whole process again

to get out of root

    exit

run to verify its active machine bash is linked to if you see the star on active you in the right

    docker-machine ls

might have to exit and sudo back in and start from the beginning of this division to get it to work

go to the folder containg the -compose.yml and run
    docker stack deploy -c docker-compose.yml getstartedlab

you see only the nodes in your swarm are manipulating the service request process
    docker stack ps getstartedlab

adding things is the same only when you want your computers to take on the workload use
    docker stack deploy


-----------------------------Cleanup and reboot

take down the swarm
    docker stack rm getstartedlab

to unset your ssh vm connection
    eval $(docker-machine env -u)

If you shut down your local host, Docker machines stops running.To restart a machine that’s stopped, run:
    docker-machine start [machine-name]
but results can be
 Unable to query docker version: Get https://192.168.99.104:2376/v1.15/version: x509: certificate signed by unknown authority
when i ran sudo docker-machine env myvm1 told me to do this
    docker-machine regenerate-certs [name]
but idk what name refers to so ill skip here

also when restarting machines, restart in the order you first started or the signing keys will be incorrect for the IP


--------------------------------------------------Part 5: Stacks-------------------------------------------------

A stack is a group of interrelated services that share dependencies, and can be orchestrated and scaled together.


                             add a free visualizer service

    open up docker-compose.yml and replace the file

a volumes key, giving the visualizer access to the host’s socket file for Docker,
placement key, ensuring that this service only ever runs on a swarm manager – never a worke

    rerun docker stack deploy -c docker-compose.yml getstartedlab

    look at the vizualizer at port 8080 for the respective IP

The visualizer is a standalone service that can run in any app that includes it in the stack.


                            Persist the data (a service that does have a dependency)

    open up docker-compose.yml and replace the file

Redis has an official image in the Docker library and has been granted the short image name of just redis, so no username/repo notation here.

The Redis port, 6379, has been pre-configured by Redis to be exposed from the container to the host, and here in our Compose file we expose it from the host to the world, can use any of swarm node ip to manage redis

redis always runs on the manager, so it’s always using the same filesystem.
redis accesses an arbitrary directory in the host’s file system as /data inside the container, which is where Redis stores data.
    this creates  “source of truth”
The placement constraint you put on the Redis service, ensuring that it always uses the same host.
The volume you created that lets the container access ./data (on the host) as /data (inside the Redis container). While containers come and go, the files stored on ./data on the specified host persists, enabling continuity.





                            Create a ./data directory on the manager:
            you have to literally make it in the vm using
            docker-machine --native-ssh  ssh [VM MACHINE NAME] "[command] "

                            run docker stack deploy -c docker-compose.yml getstartedlab
                           sudo docker stack deploy -c docker-compose.yml getstartedlab

apparentely docker and sudo docker do different things
while on enb
sudo docker makes elgesis do the work and it seems to make it a swarm manager at the same time

it seems that redis does not have permission to run under we have to configure this
playing around it only now sees redis and nothing else
seems like you have to wait
                            Run docker service ls to verify that the three services are running as expected.



-------------------------------------------Part 6: Deploy your app-------------------------------------------------

                    To set up and deploy:

Connect Docker Cloud with your preferred provider, granting Docker Cloud permission to automatically provision and “Dockerize” VMs for you.
Use Docker Cloud to create your computing resources and create your swarm.
Deploy your app.


//choose    Use the Docker Cloud Agent to bring your own host
    // error yr linux distro is not tested
    // went to make an Microsoft azure acct
    // said my gmail acct was associated with it going to fix that,
    // made a AWS acct halfway
    // seems to be that there is scammary on all sides, so this is where my studies of docker ends, I have learned plenty